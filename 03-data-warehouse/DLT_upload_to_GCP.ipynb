{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC2QnhmKxpq1"
   },
   "source": [
    "**Please set up your credentials JSON as GCP_CREDENTIALS secrets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/me/tmp/DataTalks/znyiri100/data-engineering-zoomcamp/03-data-warehouse/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "UsUZobVduL7l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"DESTINATION__CREDENTIALS\"] = userdata.get(\"GCP_CREDENTIALS\")\n",
    "# os.environ[\"BUCKET_URL\"] = \"gs://your_bucket_url\"\n",
    "\n",
    "# Load from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# In your .env file, add:\n",
    "# DESTINATION__CREDENTIALS=<your-json-credentials-as-string>\n",
    "# BUCKET_URL=gs://your_bucket_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://zoomcamp_bucket_kestra-sandbox-8656\n"
     ]
    }
   ],
   "source": [
    "!echo $BUCKET_URL #$DESTINATION__CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mPBzsEgyjsBo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/me/tmp/DataTalks/znyiri100/data-engineering-zoomcamp/03-data-warehouse/.venv/bin/python\n",
      "\u001b[2mResolved \u001b[1m165 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m159 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "Name: dlt\n",
      "Version: 1.21.0\n",
      "Location: /home/me/tmp/DataTalks/znyiri100/data-engineering-zoomcamp/03-data-warehouse/.venv/lib/python3.11/site-packages\n",
      "Requires: click, fsspec, gitpython, giturlparse, humanize, jsonpath-ng, orjson, packaging, pathvalidate, pendulum, pluggy, pytz, pyyaml, requests, requirements-parser, rich-argparse, semver, setuptools, simplejson, sqlglot, tenacity, tomlkit, typing-extensions, tzdata\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "# Install for production\n",
    "# %%capture\n",
    "# !pip install dlt[bigquery, gs]\n",
    "\n",
    "!uv run which python\n",
    "!uv add dlt[bigquery,gs,duckdb]\n",
    "!uv pip show dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "evdUsDNbkCTk"
   },
   "outputs": [],
   "source": [
    "# Install for testing\n",
    "# %%capture\n",
    "# !pip install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lYh7r1mTf4uo"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dlt.destinations import filesystem\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76zT1PzAgs7A"
   },
   "source": [
    "Ingesting parquet files to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "xya0215jsnsb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-07 06:17:46,532|[WARNING]|1908999|128538107901760|dlt|pipeline.py|_state_to_props:1731|The destination dlt.destinations.duckdb:None in state differs from destination dlt.destinations.filesystem:filesystem in pipeline and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2964624, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
      "2 (3007526, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\n",
      "3 (3582628, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet\n",
      "4 (3514289, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\n",
      "5 (3723833, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet\n",
      "6 (3539193, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\n",
      "Pipeline rides_pipeline load step completed in 30.17 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset rides_dataset\n",
      "The filesystem destination used gs://zoomcamp_bucket_kestra-sandbox-8656 location to store data\n",
      "Load package 1770463097.7839444 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Define a dlt source to download and process Parquet files as resources\n",
    "@dlt.source(name=\"rides\")\n",
    "def download_parquet():\n",
    "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
    "    for month in range(1, 7):\n",
    "    # for month in range(1, 3):        \n",
    "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
    "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        df = pd.read_parquet(BytesIO(response.content))\n",
    "        print(month,df.shape, url)\n",
    "\n",
    "        # Return the dataframe as a dlt resource for ingestion\n",
    "        yield dlt.resource(df, name=file_name)\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline\",\n",
    "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
    "    dataset_name=\"rides_dataset\",\n",
    ")\n",
    "\n",
    "# Run the pipeline to load Parquet data into DuckDB\n",
    "load_info = pipeline.run(download_parquet(), loader_file_format=\"parquet\")\n",
    "\n",
    "# # Print the results\n",
    "print(load_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0310FT-gy_P"
   },
   "source": [
    "Ingesting data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://zoomcamp_bucket_kestra-sandbox-8656\n"
     ]
    }
   ],
   "source": [
    "# Unset the credentials for DuckDB testing\n",
    "if \"DESTINATION__CREDENTIALS\" in os.environ:\n",
    "    del os.environ[\"DESTINATION__CREDENTIALS\"]\n",
    "    \n",
    "!echo $BUCKET_URL $DESTINATION__CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_3K97w1c2v2",
    "outputId": "4b2d26bf-2814-46fa-f80d-7a2e17417a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2964624, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
      "2 (3007526, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\n",
      "3 (3582628, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet\n",
      "4 (3514289, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\n",
      "5 (3723833, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet\n",
      "6 (3539193, 19) https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\n",
      "Pipeline rides_pipeline load step completed in 10.42 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset rides_dataset\n",
      "The duckdb destination used duckdb:////home/me/tmp/DataTalks/znyiri100/data-engineering-zoomcamp/03-data-warehouse/rides_pipeline.duckdb location to store data\n",
      "Load package 1770462775.2709737 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Define a dlt resource to download and process Parquet files as single table\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def download_parquet():\n",
    "    prefix = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata'\n",
    "\n",
    "    for month in range(1, 7):\n",
    "    # for month in range(1, 2):        \n",
    "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        df = pd.read_parquet(BytesIO(response.content))\n",
    "        print(month,df.shape, url)\n",
    "\n",
    "        yield df\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline\",\n",
    "    destination=\"duckdb\",  # Use DuckDB for testing\n",
    "    # destination=\"bigquery\",  # Use BigQuery for production\n",
    "    dataset_name=\"rides_dataset\",\n",
    ")\n",
    "\n",
    "# Run the pipeline to load Parquet data into DuckDB\n",
    "info = pipeline.run(download_parquet)\n",
    "\n",
    "# Print the results\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDcLjzLtooBV",
    "outputId": "74ff2de7-2f2e-41b9-a681-3dc5887f6eed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rides_pipeline</td>\n",
       "      <td>rides_dataset</td>\n",
       "      <td>_dlt_loads</td>\n",
       "      <td>[load_id, schema_name, status, inserted_at, sc...</td>\n",
       "      <td>[VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rides_pipeline</td>\n",
       "      <td>rides_dataset</td>\n",
       "      <td>_dlt_pipeline_state</td>\n",
       "      <td>[version, engine_version, pipeline_name, state...</td>\n",
       "      <td>[BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP W...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rides_pipeline</td>\n",
       "      <td>rides_dataset</td>\n",
       "      <td>_dlt_version</td>\n",
       "      <td>[version, engine_version, inserted_at, schema_...</td>\n",
       "      <td>[BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VAR...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rides_pipeline</td>\n",
       "      <td>rides_dataset</td>\n",
       "      <td>rides</td>\n",
       "      <td>[vendor_id, tpep_pickup_datetime, tpep_dropoff...</td>\n",
       "      <td>[INTEGER, TIMESTAMP WITH TIME ZONE, TIMESTAMP ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         database         schema                 name  \\\n",
       "0  rides_pipeline  rides_dataset           _dlt_loads   \n",
       "1  rides_pipeline  rides_dataset  _dlt_pipeline_state   \n",
       "2  rides_pipeline  rides_dataset         _dlt_version   \n",
       "3  rides_pipeline  rides_dataset                rides   \n",
       "\n",
       "                                        column_names  \\\n",
       "0  [load_id, schema_name, status, inserted_at, sc...   \n",
       "1  [version, engine_version, pipeline_name, state...   \n",
       "2  [version, engine_version, inserted_at, schema_...   \n",
       "3  [vendor_id, tpep_pickup_datetime, tpep_dropoff...   \n",
       "\n",
       "                                        column_types  temporary  \n",
       "0  [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME...      False  \n",
       "1  [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP W...      False  \n",
       "2  [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VAR...      False  \n",
       "3  [INTEGER, TIMESTAMP WITH TIME ZONE, TIMESTAMP ...      False  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "\n",
    "# Set search path to the dataset\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "\n",
    "# Describe the dataset to see loaded tables\n",
    "res = conn.sql(\"DESCRIBE\").df()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVJy8JoerI2P",
    "outputId": "3f8c7fee-a9ee-4fd4-ec75-153ca60bd36f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(1)\n",
      "0  20332093\n"
     ]
    }
   ],
   "source": [
    "# provide a resource name to query a table of that name\n",
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
    "        data = cursor.df()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
